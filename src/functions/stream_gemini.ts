import { GSContext, GSStatus } from '@godspeedsystems/core';
import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { StateGraph, END, MemorySaver, Annotation } from '@langchain/langgraph';
import { ToolNode } from '@langchain/langgraph/prebuilt';
import { RAGPipeline } from '../helper/mcpRag';
import { tool } from '@langchain/core/tools';
import { z } from 'zod';
import { BaseMessage } from '@langchain/core/messages';
import { HumanMessage, SystemMessage } from '@langchain/core/messages';
import { getPrompts } from '../helper/prompts';
import { memorySaver } from '../helper/memory';

export const seenThreads = new Set<string>();

export default async function stream_gemini(ctx: GSContext): Promise<GSStatus> {
  const { ws, clientId, payload } = ctx.inputs.data;

  if (!ws || ws.readyState !== ws.OPEN) {
    ctx.logger.error(`WebSocket not connected: ${clientId}`);
    return new GSStatus(false, 400, 'WebSocket disconnected');
  }

  // STEP 1: Load VectorStore + Create RAG Tool

  const GraphState = Annotation.Root({
    messages: Annotation<BaseMessage[]>({
      reducer: (x, y) => x.concat(y),
      default: () => [],
    }),
  });
  const ragTool = tool(
    async (input) => {
      const rag = new RAGPipeline()
      const result = await rag.run(input.query);
      return result;
    },
    {
      name: 'get_relevant_docs',
      description: 'Call to get revelant documents from user query.',
      schema: z.object({
        query: z.string().describe('User query to get relevant docs.'),
      }),
    },
  );

  // STEP 2: Create the LangGraph LLM Agent with Tool Support

  const toolnode = new ToolNode<typeof GraphState.State>([ragTool]);

  async function shouldRetrieve(
    state: typeof GraphState.State,
  ): Promise<string> {
    const { messages } = state;
    ctx.logger.info('---DECIDE TO RETRIEVE---');
    const lastMessage = messages[messages.length - 1];

    if (
      'tool_calls' in lastMessage &&
      Array.isArray(lastMessage.tool_calls) &&
      lastMessage.tool_calls.length
    ) {
      ctx.logger.info('---DECISION: RETRIEVE---');
      return 'tools';
    }
    // If there are no tool calls then we finish.
    ctx.logger.info('---DECISION: FINISH---');
    return END;
  }

  async function agent(
    state: typeof GraphState.State,
  ): Promise<Partial<typeof GraphState.State>> {
    ctx.logger.info('---CALL AGENT---');

    const { messages } = state;

    const llm = new ChatGoogleGenerativeAI({
      model: process.env.GEMINI_MODEL || 'gemini-2.0-flash',
      temperature: 0.7,
      streaming: true,
    }).bindTools([ragTool]);

    const response = await llm.invoke(messages);
    return {
      messages: [response],
    };
  }

  const graph = new StateGraph(GraphState)
    .addNode('agent', agent)
    .addNode('tools', toolnode);

  graph.addEdge('__start__', 'agent');
  graph.addConditionalEdges('agent', shouldRetrieve);
  graph.addEdge('tools', 'agent');

  const { core_system_prompt, tool_knowledge_prompt } = getPrompts();
  const systemPromot = Array(core_system_prompt, tool_knowledge_prompt).join(
    '\n',
  );

  const runnable = graph.compile({
    checkpointer: memorySaver,
  });

  const threadId = clientId;
  const messages: BaseMessage[] = [];

  if (!seenThreads.has(threadId)) {
    messages.push(new SystemMessage(systemPromot));
    seenThreads.add(threadId);
  }

  messages.push(new HumanMessage(payload.message));

  try {
    let streamStarted = false;
    await runnable.stream(
      { messages },
      {
        configurable: {
          thread_id: threadId,
        },
        callbacks: [
          {
            handleToolStart: async (tool, input) => {
              ws.send(
                JSON.stringify({
                  eventtype: 'stream.start',
                  payload: { message: '[STREAM_START]' },
                }),
              );
              ws.send(
                JSON.stringify({
                  eventtype: 'stream.chunk',
                  payload: {
                    message: 'Please wait...Retreving relevant documents.',
                  },
                }),
              );
              ws.send(
                JSON.stringify({
                  eventtype: 'stream.end',
                  payload: { message: '[STREAM_END]' },
                }),
              );
            },
            handleLLMStart: async () => {
              if (!streamStarted) {
                streamStarted = true;
                ws.send(
                  JSON.stringify({
                    eventtype: 'stream.start',
                    payload: { message: '[STREAM_START]' },
                  }),
                );
              }
            },
            handleLLMNewToken: async (token) => {
              if (token.length > 0) {
                ws.send(
                  JSON.stringify({
                    eventtype: 'stream.chunk',
                    payload: { message: token },
                  }),
                );
              }
            },
            handleLLMEnd: async () => {
              ws.send(
                JSON.stringify({
                  eventtype: 'stream.end',
                  payload: { message: '[STREAM_END]' },
                }),
              );
              streamStarted = false;
            },
          },
        ],
      },
    );

    ctx.logger.info(`Completed streaming for ${clientId}`);
    return new GSStatus(true, 200, 'Streaming completed');
  } catch (err: any) {
    ctx.logger.error(`LangGraph streaming error: ${err.message}`);
    ws.send(
      JSON.stringify({ eventtype: 'error', payload: { message: '[ERROR]' } }),
    );
    return new GSStatus(false, 500, 'Streaming failed');
  }
}
